{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite - \n",
    "\n",
    "**For demo purposes, run redis locally**\n",
    "\n",
    "docker run -d -p 6379:6379 redislabs/redismod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading file: 2010.15980.json\n",
      "\n",
      "\n",
      "Loading file: 2101.00190.json\n",
      "\n",
      "\n",
      "Loading file: 2104.08691.json\n",
      "\n",
      "\n",
      "Loading file: 2110.08387.json\n",
      "\n",
      "\n",
      "Loading file: 2201.11903.json\n",
      "\n",
      "\n",
      "Loading file: 2203.11171.json\n",
      "\n",
      "\n",
      "Loading file: 2211.01910.json\n",
      "\n",
      "\n",
      "Loading file: 2212.10496.json\n"
     ]
    }
   ],
   "source": [
    "from AzureOpenAIUtil.Embedding import DocumentEmbedding\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "doc_emb = DocumentEmbedding(document_embedding_deployment_name=os.getenv('DOCUMENT_MODEL_NAME'),\\\n",
    "                            query_embedding_deployment_name=os.getenv('QUERY_MODEL_NAME'),\\\n",
    "                            summerization_deployment_name=os.getenv('DEPLOYMENT_NAME'))\n",
    "doc_emb.load_documents('data/extracted/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result{2 total, docs: [Document {'id': 'docuemnt:2211.01910.pdf-15', 'payload': None, 'vector_score': '0.571007072926', 'document_name': '2211.01910.pdf', 'page_number': '15', 'page_text': 'A PROMPT ENGINEERING IN THE WILD Large models with natural language interfaces, including models for text generation and image synthesis, have seen an increasing amount of public usage in recent years. As finding the right prompt can be difficult for humans, a number of guides on prompt engineering as well as tools to aid in prompt discovery have been developed. Among others, see, for example: • https://blog.andrewcantino.com/blog/2021/04/21/prompt-engineering-tips-and-tricks/ • https://techcrunch.com/2022/07/29/a-startup-is-charging-1-99-for-strings-of-text-to-feed-to-dall-e-2/ • https://news.ycombinator.com/item?id=32943224 • https://promptomania.com/stable-diffusion-prompt-builder/ • https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion In this paper we apply APE to generate effective instructions for steering LLMs, but the general framework Algorithm 1 could be applied to steer other models with natural language interfaces so long as an appropriate proposal method and scoring function can be designed. 15'}, Document {'id': 'docuemnt:2211.01910.pdf-1', 'payload': None, 'vector_score': '0.576043069363', 'document_name': '2211.01910.pdf', 'page_number': '1', 'page_text': 'LARGE LANGUAGE PROMPT ENGINEERS MODELS ARE HUMAN-LEVEL Yongchao Zhou∗,1,2 Andrei Ioan Muresanu∗,2,3 Silviu Pitis1,2 Harris Chan1,2 1University of Toronto 2Vector Institute 3University of Waterloo ABSTRACT Ziwen Han∗,1,2 Jimmy Ba1,2 Keiran Paster1,2 By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the “program,” optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer. 1 1 INTRODUCTION The combination of scale and attention-based architectures has resulted in language models possessing an unprecedented level of generality (Kaplan et al., 2020; Vaswani et al., 2017). These so-called “large language models” (LLMs) have shown remarkable, often superhuman, capabilities across a diverse range of tasks, including both zero-shot and few-shot setups (Brown et al., 2020; Srivastava et al., 2022). With generality, however, there comes a question of control: how can we make LLMs do what we want them to do? To answer this question and steer LLMs toward desired behaviors, recent work has considered fine-tuning (Ouyang et al., 2022; Ziegler et al., 2019), in-context learning (Brown et al., 2020), and several forms of prompt generation (Gao, 2021), including both differentiable tuning of soft prompts (Qin & Eisner, 2021; Lester et al., 2021) and natural language prompt engineering (Reynolds & McDonell, 2021). The latter is of particular interest, as it provides a natural interface for humans to communicate with machines and may be of great relevance not only to LLMs but to other generalist models such as prompted image synthesizers (Rombach et al., 2022; Ramesh et al., 2022), for which public interest in prompt design and generation has also emerged (see Appendix A for examples). Behind this interest is the fact that plain language prompts do not always produce the desired results, even when those results are possible to produce with alternative instructions. Thus, human users must experiment with a wide range of prompts to elicit desired behaviors, as they have little knowledge of how compatible instructions are with a particular model. We can understand this by viewing LLMs as black-box computers that execute programs specified by natural language instructions: while they can execute a broad range of natural language programs, the way these programs are processed may not be intuitive for humans, and the quality of instruction can only be measured when executing these instructions on a downstream task (Sanh et al., 2022; Wei et al., 2021). To reduce the human effort involved in creating and validating effective instructions, we propose a novel algorithm using LLMs to generate and select instructions automatically. We call this problem * Equal contribution. Corresponding email: yczhou@cs.toronto.edu 1 Our code is available at https://github.com/keirp/automatic_prompt_engineer. 1 arXiv:2211.01910v1 [cs.LG] 3 Nov 2022'}]}\n",
      "[Document(page_content='A PROMPT ENGINEERING IN THE WILD Large models with natural language interfaces, including models for text generation and image synthesis, have seen an increasing amount of public usage in recent years. As finding the right prompt can be difficult for humans, a number of guides on prompt engineering as well as tools to aid in prompt discovery have been developed. Among others, see, for example: • https://blog.andrewcantino.com/blog/2021/04/21/prompt-engineering-tips-and-tricks/ • https://techcrunch.com/2022/07/29/a-startup-is-charging-1-99-for-strings-of-text-to-feed-to-dall-e-2/ • https://news.ycombinator.com/item?id=32943224 • https://promptomania.com/stable-diffusion-prompt-builder/ • https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion In this paper we apply APE to generate effective instructions for steering LLMs, but the general framework Algorithm 1 could be applied to steer other models with natural language interfaces so long as an appropriate proposal method and scoring function can be designed. 15', lookup_str='', metadata={'source': '2211.01910.pdf:15'}, lookup_index=0), Document(page_content='LARGE LANGUAGE PROMPT ENGINEERS MODELS ARE HUMAN-LEVEL Yongchao Zhou∗,1,2 Andrei Ioan Muresanu∗,2,3 Silviu Pitis1,2 Harris Chan1,2 1University of Toronto 2Vector Institute 3University of Waterloo ABSTRACT Ziwen Han∗,1,2 Jimmy Ba1,2 Keiran Paster1,2 By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the “program,” optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer. 1 1 INTRODUCTION The combination of scale and attention-based architectures has resulted in language models possessing an unprecedented level of generality (Kaplan et al., 2020; Vaswani et al., 2017). These so-called “large language models” (LLMs) have shown remarkable, often superhuman, capabilities across a diverse range of tasks, including both zero-shot and few-shot setups (Brown et al., 2020; Srivastava et al., 2022). With generality, however, there comes a question of control: how can we make LLMs do what we want them to do? To answer this question and steer LLMs toward desired behaviors, recent work has considered fine-tuning (Ouyang et al., 2022; Ziegler et al., 2019), in-context learning (Brown et al., 2020), and several forms of prompt generation (Gao, 2021), including both differentiable tuning of soft prompts (Qin & Eisner, 2021; Lester et al., 2021) and natural language prompt engineering (Reynolds & McDonell, 2021). The latter is of particular interest, as it provides a natural interface for humans to communicate with machines and may be of great relevance not only to LLMs but to other generalist models such as prompted image synthesizers (Rombach et al., 2022; Ramesh et al., 2022), for which public interest in prompt design and generation has also emerged (see Appendix A for examples). Behind this interest is the fact that plain language prompts do not always produce the desired results, even when those results are possible to produce with alternative instructions. Thus, human users must experiment with a wide range of prompts to elicit desired behaviors, as they have little knowledge of how compatible instructions are with a particular model. We can understand this by viewing LLMs as black-box computers that execute programs specified by natural language instructions: while they can execute a broad range of natural language programs, the way these programs are processed may not be intuitive for humans, and the quality of instruction can only be measured when executing these instructions on a downstream task (Sanh et al., 2022; Wei et al., 2021). To reduce the human effort involved in creating and validating effective instructions, we propose a novel algorithm using LLMs to generate and select instructions automatically. We call this problem * Equal contribution. Corresponding email: yczhou@cs.toronto.edu 1 Our code is available at https://github.com/keirp/automatic_prompt_engineer. 1 arXiv:2211.01910v1 [cs.LG] 3 Nov 2022', lookup_str='', metadata={'source': '2211.01910.pdf:1'}, lookup_index=0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='A PROMPT ENGINEERING IN THE WILD Large models with natural language interfaces, including models for text generation and image synthesis, have seen an increasing amount of public usage in recent years. As finding the right prompt can be difficult for humans, a number of guides on prompt engineering as well as tools to aid in prompt discovery have been developed. Among others, see, for example: • https://blog.andrewcantino.com/blog/2021/04/21/prompt-engineering-tips-and-tricks/ • https://techcrunch.com/2022/07/29/a-startup-is-charging-1-99-for-strings-of-text-to-feed-to-dall-e-2/ • https://news.ycombinator.com/item?id=32943224 • https://promptomania.com/stable-diffusion-prompt-builder/ • https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion In this paper we apply APE to generate effective instructions for steering LLMs, but the general framework Algorithm 1 could be applied to steer other models with natural language interfaces so long as an appropriate proposal method and scoring function can be designed. 15', lookup_str='', metadata={'source': '2211.01910.pdf:15'}, lookup_index=0),\n",
       "  Document(page_content='LARGE LANGUAGE PROMPT ENGINEERS MODELS ARE HUMAN-LEVEL Yongchao Zhou∗,1,2 Andrei Ioan Muresanu∗,2,3 Silviu Pitis1,2 Harris Chan1,2 1University of Toronto 2Vector Institute 3University of Waterloo ABSTRACT Ziwen Han∗,1,2 Jimmy Ba1,2 Keiran Paster1,2 By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the “program,” optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer. 1 1 INTRODUCTION The combination of scale and attention-based architectures has resulted in language models possessing an unprecedented level of generality (Kaplan et al., 2020; Vaswani et al., 2017). These so-called “large language models” (LLMs) have shown remarkable, often superhuman, capabilities across a diverse range of tasks, including both zero-shot and few-shot setups (Brown et al., 2020; Srivastava et al., 2022). With generality, however, there comes a question of control: how can we make LLMs do what we want them to do? To answer this question and steer LLMs toward desired behaviors, recent work has considered fine-tuning (Ouyang et al., 2022; Ziegler et al., 2019), in-context learning (Brown et al., 2020), and several forms of prompt generation (Gao, 2021), including both differentiable tuning of soft prompts (Qin & Eisner, 2021; Lester et al., 2021) and natural language prompt engineering (Reynolds & McDonell, 2021). The latter is of particular interest, as it provides a natural interface for humans to communicate with machines and may be of great relevance not only to LLMs but to other generalist models such as prompted image synthesizers (Rombach et al., 2022; Ramesh et al., 2022), for which public interest in prompt design and generation has also emerged (see Appendix A for examples). Behind this interest is the fact that plain language prompts do not always produce the desired results, even when those results are possible to produce with alternative instructions. Thus, human users must experiment with a wide range of prompts to elicit desired behaviors, as they have little knowledge of how compatible instructions are with a particular model. We can understand this by viewing LLMs as black-box computers that execute programs specified by natural language instructions: while they can execute a broad range of natural language programs, the way these programs are processed may not be intuitive for humans, and the quality of instruction can only be measured when executing these instructions on a downstream task (Sanh et al., 2022; Wei et al., 2021). To reduce the human effort involved in creating and validating effective instructions, we propose a novel algorithm using LLMs to generate and select instructions automatically. We call this problem * Equal contribution. Corresponding email: yczhou@cs.toronto.edu 1 Our code is available at https://github.com/keirp/automatic_prompt_engineer. 1 arXiv:2211.01910v1 [cs.LG] 3 Nov 2022', lookup_str='', metadata={'source': '2211.01910.pdf:1'}, lookup_index=0)],\n",
       " 'question': 'What is automated prompt engineering?',\n",
       " 'source': '2211.01910.pdf:1',\n",
       " 'intermediate_steps': [{'answer': ' Automated prompt engineering is a process of generating effective instructions for steering large models with natural language interfaces, such as models for text generation and image synthesis.',\n",
       "   'score': '90'},\n",
       "  {'answer': ' Automated prompt engineering is a novel algorithm using large language models to generate and select instructions automatically.',\n",
       "   'score': '100'}],\n",
       " 'output_text': ' Automated prompt engineering is a novel algorithm using large language models to generate and select instructions automatically.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_emb.query('What is automated prompt engineering?',topK=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result{2 total, docs: [Document {'id': 'docuemnt:2201.11903.pdf-1', 'payload': None, 'vector_score': '0.599919557571', 'document_name': '2201.11903.pdf', 'page_number': '1', 'page_text': 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models Jason Wei Xuezhi Wang Dale Schuurmans Brian Ichter Fei Xia Ed H. Chi Quoc V. Le Google Research, Brain Team {jasonwei,dennyzhou}@google.com Maarten Bosma Denny Zhou Abstract We explore how generating a chain of thought—a series of intermediate reasoning steps—significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-of- thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier. Standard Prompting Model Input Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? Chain-of-Thought Prompting Model Input Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? Model Output Model Output A: The answer is 27. x A: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9. Figure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic, commonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted. arXiv:2201.11903v6 [cs.CL] 10 Jan 2023 36th Conference on Neural Information Processing Systems (NeurIPS 2022).'}, Document {'id': 'docuemnt:2203.11171.pdf-1', 'payload': None, 'vector_score': '0.599938154221', 'document_name': '2203.11171.pdf', 'page_number': '1', 'page_text': '1 Google Research SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS Xuezhi Wang†‡, Jason Wei† , Dale Schuurmans† , Quoc Le† , Ed H. Chi† , Sharan Narang† , Aakanksha Chowdhery† †Google Research, Brain Team ‡xuezhiw@google.com, , Denny Zhou†§ §dennyzhou@google.com ABSTRACT Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%). 1 INTRODUCTION Although language models have demonstrated remarkable success across a range of NLP tasks, their ability to demonstrate reasoning is often seen as a limitation, which cannot be overcome solely by increasing model scale (Rae et al., 2021; BIG-bench collaboration, 2021, inter alia). In an effort to address this shortcoming, Wei et al. (2022) have proposed chain-of-thought prompting, where a language model is prompted to generate a series of short sentences that mimic the reasoning process a person might employ in solving a task. For example, given the question “If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?”, instead of directly responding with “5”, a language model would be prompted to respond with the entire chain-of-thought: “There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.”. It has been observed that chain-of-thought prompting significantly improves model performance across a variety of multi-step reasoning tasks (Wei et al., 2022). In this paper, we introduce a novel decoding strategy called self-consistency to replace the greedy decoding strategy used in chain-of-thought prompting (Wei et al., 2022), that further improves language models’ reasoning performance by a significant margin. Self-consistency leverages the intuition that complex reasoning tasks typically admit multiple reasoning paths that reach a correct answer (Stanovich & West, 2000). The more that deliberate thinking and analysis is required for a problem (Evans, 2010), the greater the diversity of reasoning paths that can recover the answer. Figure 1 illustrates the self-consistency method with an example. We first prompt the language model with chain-of-thought prompting, then instead of greedily decoding the optimal reasoning path, we propose a “sample-and-marginalize” decoding procedure: we first sample from the language model’s decoder to generate a diverse set of reasoning paths; each reasoning path might lead to a different final answer, so we determine the optimal answer by marginalizing out the sampled reasoning paths to find the most consistent answer in the final answer set. Such an approach is analogous to the human experience that if multiple different ways of thinking lead to the same answer, one has greater confidence that the final answer is correct. Compared to other decoding methods, self-consistency avoids the repetitiveness and local-optimality that plague greedy decoding, while mitigating the stochasticity of a single sampled generation. arXiv:2203.11171v3 [cs.CL] 4 Oct 2022'}]}\n",
      "[Document(page_content='Chain-of-Thought Prompting Elicits Reasoning in Large Language Models Jason Wei Xuezhi Wang Dale Schuurmans Brian Ichter Fei Xia Ed H. Chi Quoc V. Le Google Research, Brain Team {jasonwei,dennyzhou}@google.com Maarten Bosma Denny Zhou Abstract We explore how generating a chain of thought—a series of intermediate reasoning steps—significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-of- thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier. Standard Prompting Model Input Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? Chain-of-Thought Prompting Model Input Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? Model Output Model Output A: The answer is 27. x A: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9. Figure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic, commonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted. arXiv:2201.11903v6 [cs.CL] 10 Jan 2023 36th Conference on Neural Information Processing Systems (NeurIPS 2022).', lookup_str='', metadata={'source': '2201.11903.pdf:1'}, lookup_index=0), Document(page_content='1 Google Research SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS Xuezhi Wang†‡, Jason Wei† , Dale Schuurmans† , Quoc Le† , Ed H. Chi† , Sharan Narang† , Aakanksha Chowdhery† †Google Research, Brain Team ‡xuezhiw@google.com, , Denny Zhou†§ §dennyzhou@google.com ABSTRACT Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%). 1 INTRODUCTION Although language models have demonstrated remarkable success across a range of NLP tasks, their ability to demonstrate reasoning is often seen as a limitation, which cannot be overcome solely by increasing model scale (Rae et al., 2021; BIG-bench collaboration, 2021, inter alia). In an effort to address this shortcoming, Wei et al. (2022) have proposed chain-of-thought prompting, where a language model is prompted to generate a series of short sentences that mimic the reasoning process a person might employ in solving a task. For example, given the question “If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?”, instead of directly responding with “5”, a language model would be prompted to respond with the entire chain-of-thought: “There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.”. It has been observed that chain-of-thought prompting significantly improves model performance across a variety of multi-step reasoning tasks (Wei et al., 2022). In this paper, we introduce a novel decoding strategy called self-consistency to replace the greedy decoding strategy used in chain-of-thought prompting (Wei et al., 2022), that further improves language models’ reasoning performance by a significant margin. Self-consistency leverages the intuition that complex reasoning tasks typically admit multiple reasoning paths that reach a correct answer (Stanovich & West, 2000). The more that deliberate thinking and analysis is required for a problem (Evans, 2010), the greater the diversity of reasoning paths that can recover the answer. Figure 1 illustrates the self-consistency method with an example. We first prompt the language model with chain-of-thought prompting, then instead of greedily decoding the optimal reasoning path, we propose a “sample-and-marginalize” decoding procedure: we first sample from the language model’s decoder to generate a diverse set of reasoning paths; each reasoning path might lead to a different final answer, so we determine the optimal answer by marginalizing out the sampled reasoning paths to find the most consistent answer in the final answer set. Such an approach is analogous to the human experience that if multiple different ways of thinking lead to the same answer, one has greater confidence that the final answer is correct. Compared to other decoding methods, self-consistency avoids the repetitiveness and local-optimality that plague greedy decoding, while mitigating the stochasticity of a single sampled generation. arXiv:2203.11171v3 [cs.CL] 4 Oct 2022', lookup_str='', metadata={'source': '2203.11171.pdf:1'}, lookup_index=0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='Chain-of-Thought Prompting Elicits Reasoning in Large Language Models Jason Wei Xuezhi Wang Dale Schuurmans Brian Ichter Fei Xia Ed H. Chi Quoc V. Le Google Research, Brain Team {jasonwei,dennyzhou}@google.com Maarten Bosma Denny Zhou Abstract We explore how generating a chain of thought—a series of intermediate reasoning steps—significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-of- thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier. Standard Prompting Model Input Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? Chain-of-Thought Prompting Model Input Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? Model Output Model Output A: The answer is 27. x A: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9. Figure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic, commonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted. arXiv:2201.11903v6 [cs.CL] 10 Jan 2023 36th Conference on Neural Information Processing Systems (NeurIPS 2022).', lookup_str='', metadata={'source': '2201.11903.pdf:1'}, lookup_index=0),\n",
       "  Document(page_content='1 Google Research SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS Xuezhi Wang†‡, Jason Wei† , Dale Schuurmans† , Quoc Le† , Ed H. Chi† , Sharan Narang† , Aakanksha Chowdhery† †Google Research, Brain Team ‡xuezhiw@google.com, , Denny Zhou†§ §dennyzhou@google.com ABSTRACT Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%). 1 INTRODUCTION Although language models have demonstrated remarkable success across a range of NLP tasks, their ability to demonstrate reasoning is often seen as a limitation, which cannot be overcome solely by increasing model scale (Rae et al., 2021; BIG-bench collaboration, 2021, inter alia). In an effort to address this shortcoming, Wei et al. (2022) have proposed chain-of-thought prompting, where a language model is prompted to generate a series of short sentences that mimic the reasoning process a person might employ in solving a task. For example, given the question “If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?”, instead of directly responding with “5”, a language model would be prompted to respond with the entire chain-of-thought: “There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.”. It has been observed that chain-of-thought prompting significantly improves model performance across a variety of multi-step reasoning tasks (Wei et al., 2022). In this paper, we introduce a novel decoding strategy called self-consistency to replace the greedy decoding strategy used in chain-of-thought prompting (Wei et al., 2022), that further improves language models’ reasoning performance by a significant margin. Self-consistency leverages the intuition that complex reasoning tasks typically admit multiple reasoning paths that reach a correct answer (Stanovich & West, 2000). The more that deliberate thinking and analysis is required for a problem (Evans, 2010), the greater the diversity of reasoning paths that can recover the answer. Figure 1 illustrates the self-consistency method with an example. We first prompt the language model with chain-of-thought prompting, then instead of greedily decoding the optimal reasoning path, we propose a “sample-and-marginalize” decoding procedure: we first sample from the language model’s decoder to generate a diverse set of reasoning paths; each reasoning path might lead to a different final answer, so we determine the optimal answer by marginalizing out the sampled reasoning paths to find the most consistent answer in the final answer set. Such an approach is analogous to the human experience that if multiple different ways of thinking lead to the same answer, one has greater confidence that the final answer is correct. Compared to other decoding methods, self-consistency avoids the repetitiveness and local-optimality that plague greedy decoding, while mitigating the stochasticity of a single sampled generation. arXiv:2203.11171v3 [cs.CL] 4 Oct 2022', lookup_str='', metadata={'source': '2203.11171.pdf:1'}, lookup_index=0)],\n",
       " 'question': 'what is chain of thought?',\n",
       " 'source': '2201.11903.pdf:1',\n",
       " 'intermediate_steps': [{'answer': ' Chain-of-thought is a method of prompting a language model to generate a series of intermediate reasoning steps in order to perform complex reasoning.',\n",
       "   'score': '100'},\n",
       "  {'answer': ' Chain-of-thought prompting is a method where a language model is prompted to generate a series of short sentences that mimic the reasoning process a person might employ in solving a task.',\n",
       "   'score': '90'}],\n",
       " 'output_text': ' Chain-of-thought is a method of prompting a language model to generate a series of intermediate reasoning steps in order to perform complex reasoning.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_emb.query('what is chain of thought?', topK=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result{5 total, docs: [Document {'id': 'docuemnt:2211.01910.pdf-15', 'payload': None, 'vector_score': '0.576337456703', 'document_name': '2211.01910.pdf', 'page_number': '15', 'page_text': 'A PROMPT ENGINEERING IN THE WILD Large models with natural language interfaces, including models for text generation and image synthesis, have seen an increasing amount of public usage in recent years. As finding the right prompt can be difficult for humans, a number of guides on prompt engineering as well as tools to aid in prompt discovery have been developed. Among others, see, for example: • https://blog.andrewcantino.com/blog/2021/04/21/prompt-engineering-tips-and-tricks/ • https://techcrunch.com/2022/07/29/a-startup-is-charging-1-99-for-strings-of-text-to-feed-to-dall-e-2/ • https://news.ycombinator.com/item?id=32943224 • https://promptomania.com/stable-diffusion-prompt-builder/ • https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion In this paper we apply APE to generate effective instructions for steering LLMs, but the general framework Algorithm 1 could be applied to steer other models with natural language interfaces so long as an appropriate proposal method and scoring function can be designed. 15'}, Document {'id': 'docuemnt:2211.01910.pdf-1', 'payload': None, 'vector_score': '0.589745938778', 'document_name': '2211.01910.pdf', 'page_number': '1', 'page_text': 'LARGE LANGUAGE PROMPT ENGINEERS MODELS ARE HUMAN-LEVEL Yongchao Zhou∗,1,2 Andrei Ioan Muresanu∗,2,3 Silviu Pitis1,2 Harris Chan1,2 1University of Toronto 2Vector Institute 3University of Waterloo ABSTRACT Ziwen Han∗,1,2 Jimmy Ba1,2 Keiran Paster1,2 By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the “program,” optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer. 1 1 INTRODUCTION The combination of scale and attention-based architectures has resulted in language models possessing an unprecedented level of generality (Kaplan et al., 2020; Vaswani et al., 2017). These so-called “large language models” (LLMs) have shown remarkable, often superhuman, capabilities across a diverse range of tasks, including both zero-shot and few-shot setups (Brown et al., 2020; Srivastava et al., 2022). With generality, however, there comes a question of control: how can we make LLMs do what we want them to do? To answer this question and steer LLMs toward desired behaviors, recent work has considered fine-tuning (Ouyang et al., 2022; Ziegler et al., 2019), in-context learning (Brown et al., 2020), and several forms of prompt generation (Gao, 2021), including both differentiable tuning of soft prompts (Qin & Eisner, 2021; Lester et al., 2021) and natural language prompt engineering (Reynolds & McDonell, 2021). The latter is of particular interest, as it provides a natural interface for humans to communicate with machines and may be of great relevance not only to LLMs but to other generalist models such as prompted image synthesizers (Rombach et al., 2022; Ramesh et al., 2022), for which public interest in prompt design and generation has also emerged (see Appendix A for examples). Behind this interest is the fact that plain language prompts do not always produce the desired results, even when those results are possible to produce with alternative instructions. Thus, human users must experiment with a wide range of prompts to elicit desired behaviors, as they have little knowledge of how compatible instructions are with a particular model. We can understand this by viewing LLMs as black-box computers that execute programs specified by natural language instructions: while they can execute a broad range of natural language programs, the way these programs are processed may not be intuitive for humans, and the quality of instruction can only be measured when executing these instructions on a downstream task (Sanh et al., 2022; Wei et al., 2021). To reduce the human effort involved in creating and validating effective instructions, we propose a novel algorithm using LLMs to generate and select instructions automatically. We call this problem * Equal contribution. Corresponding email: yczhou@cs.toronto.edu 1 Our code is available at https://github.com/keirp/automatic_prompt_engineer. 1 arXiv:2211.01910v1 [cs.LG] 3 Nov 2022'}, Document {'id': 'docuemnt:2010.15980.pdf-2', 'payload': None, 'vector_score': '0.591618299484', 'document_name': '2010.15980.pdf', 'page_number': '2', 'page_text': 'Original Input Cinp a real joy. AUTOPROMPT Iprompt a real joy. atmosphere alot dialogue Clone totally [MASK]. Trigger Tokens Itrig atmosphere, alot, dialogue, Clone... Masked LM p([MASK]|æprompt) Cris marvelous philanthrop Template X(Kinp, \"\\'trig) {sentence}[T][T][T][T][T][P]. p(y x prompt) + positive worse incompetence Worse + negative Figure 1: Illustration of AUTOPROMPT applied to probe a masked language model’s (MLM’s) ability to per- form sentiment analysis. Each input, xinp, is placed into a natural language prompt, xprompt, which contains a single [MASK] token. The prompt is created using a template, λ, which combines the original input with a set of trigger tokens, xtrig. The trigger tokens are shared across all inputs and determined using a gradient-based search (Section 2.2). Probabilities for each class label, y, are then obtained by marginalizing the MLM predictions, p([MASK]|xprompt), over sets of automatically detected label tokens (Section 2.3). vides a lower bound on what the model “knows”, and is therefore a more useful analysis tool. How- ever, prompting unfortunately requires manually crafting the context to feed into the model. Not only is this time consuming and non-intuitive for many tasks (e.g., textual entailment), more impor- tantly, models are highly sensitive to this context: improperly-constructed contexts cause artificially low performance (Jiang et al., 2020). Overcoming the need to manually specify prompts would make prompting a more widely useful analysis tool. In this paper, we introduce AUTOPROMPT—an automated method for generating prompts for any task, illustrated in Figure 1. Given a task, e.g., sen- timent analysis, AUTOPROMPT creates a prompt by combining the original task inputs (e.g. reviews) with a collection of trigger tokens according to a template. The same set of trigger tokens is used for all inputs, and is learned using a variant of the gradient-based search strategy proposed in Wallace et al. (2019). The LM predictions for the prompt are converted to class probabilities by marginal- izing over a set of associated label tokens, which can either be learned or specified ahead of time, enabling the LM to be evaluated the same as one would any other classifier. We validate the effectiveness of AUTOPROMPT in numerous experiments. First, we use AUTO- PROMPT to construct prompts that test pretrained masked language models (MLMs) on sentiment analysis and natural language inference (NLI). Our tests reveal that, without any finetuning, MLMs perform well on both of these tasks—a properly- prompted RoBERTa achieves 91% accuracy on SST-2 (better than a finetuned ELMo model (Pe- ters et al., 2018)), and 69% accuracy on a bal- anced variant of the SICK-E dataset (Marelli et al., 2014). Next, we apply AUTOPROMPT to the fact re- trieval tasks of LAMA (Petroni et al., 2019), where we are able to construct prompts that more effec- tively elicit MLM’s factual knowledge than exist- ing prompts generated using manual and corpus- mining methods. Concretely, we achieve 43.3% precision-at-1, compared to the current best single- prompt result of 34.1% (Jiang et al., 2020). We also introduce a variant of this task, similar to rela- tion extraction (RE), that tests whether MLMs can extract knowledge from a given piece of text. We show that MLMs can actually outperform existing RE models when context sentences with real facts are provided, however, they struggle when context sentences are artificially falsified. Finally, although the goal of AUTOPROMPT is to analyze models, we find that it provides certain practical advantages over finetuning. First, AU- TOPROMPT achieves higher average- and worst- case accuracy than finetuning in low-data regimes. Moreover, unlike finetuning, prompting LMs does not require large amounts of disk space to store model checkpoints; once a prompt is found, it can be used on off-the-shelf pretrained LMs. This is beneficial when serving models for multiple tasks. 2 Overview of AUTOPROMPT A natural way to elicit knowledge from pretrained LMs is to pose tasks as fill-in-the-blank problems.'}, Document {'id': 'docuemnt:2010.15980.pdf-9', 'payload': None, 'vector_score': '0.593228578568', 'document_name': '2010.15980.pdf', 'page_number': '9', 'page_text': 'Model Perturbed Original Supervised RE LSTM 57.95 90.73 56.43 BERT (AUTOPROMPT) RoBERTa (AUTOPROMPT) 60.33 28.95 58.81 BERT (LAMA) 69.06 28.02 BERT (LPAQA) 76.55 30.79 Table 5: Relation Extraction: We use prompts to test pretrained MLMs on relation extraction. Compared to a state-of-the-art LSTM model from 2017, MLMs have higher mean precision-at-1 (P@1), especially when us- ing prompts from AUTOPROMPT. We also test models on sentences that have been edited to contain incorrect facts. The accuracy of MLMs drops significantly on these sentences, indicating that their high performance stems from their factual knowledge. 7 Discussion Prompting as an Alternative to Finetuning The goal of prompting a language model is to probe the knowledge that the model acquired from pre- training. Nevertheless, prompting has some prac- tical advantages over finetuning for solving real- world tasks. First, as shown in Section 3, prompts generated using AUTOPROMPT can achieve higher accuracy than finetuning in the low-data regime. Moreover, prompting has advantages over finetun- ing when trying to solve many different tasks (e.g., the many users of the OpenAI GPT-3 API (Brown et al., 2020)). In particular, finetuning requires storing large language model checkpoints for each individual task, and drastically increases system cost and complexity because it requires deploying many different models at the same time. Prompt- ing alleviates both of these issues. Only prompts are stored for each individual task, while the same pretrained model is used across all of the tasks. Limitations of Prompting There are certain phenomena that are difficult to elicit from pre- trained language models via prompts. In our pre- liminary evaluation on datasets such as QQP (Iyer et al., 2017) and RTE (Dagan et al., 2005), prompts generated manually and with AUTOPROMPT did not perform considerably better than chance. How- ever, we cannot conclude that BERT does not know paraphrasing or entailment from these results. In general, different probing methods have different tasks and phenomena they are suitable for: AUTO- PROMPT makes prompt-based probes more gener- ally applicable, but, it still remains just one tool in the toolbox of the interpretability researcher. Limitations of AUTOPROMPT One downside of AUTOPROMPT is that it requires labeled train- ing data. Although this is also required for other probing techniques (e.g., linear probing classi- fiers), manual prompts rely on domain/language insights instead of labeled data. Compared to human-designed prompts, AUTOPROMPT gener- ated prompts lack interpretability, which is similar to other probing techniques, such as linear probing classifiers. Another limitation of AUTOPROMPT is that it can sometimes struggle when the training data is highly imbalanced. For example, in Sec- tions 4 and 5 we show that the prompts often just increase the likelihood of the majority label. Re- balancing the training data can help to mitigate this problem. Finally, due to the greedy search over the large discrete space of phrases, AUTOPROMPT is sometimes brittle; we leave more effective crafting techniques for future directions. 8 Conclusion In this paper, we introduce AUTOPROMPT, an approach to develop automatically-constructed prompts that elicit knowledge from pretrained MLMs for a variety of tasks. We show that these prompts outperform manual prompts while requir- ing less human effort. Furthermore, the results for sentiment analysis and textual entailment sug- gest that, in some data-scarce settings, it may be more effective to prompt language models than to finetune them for the task. Although we fo- cus only on masked language models in this paper, our method can be trivially extended to standard language models, and thus maybe useful for con- structing inputs for models like GPT-3 (Brown et al., 2020). Source code and datasets to re- produce the results in this paper is available at http://ucinlp.github.io/autoprompt. Acknowledgments We would like to thank the LAMA and LPAQA teams for answering our questions. We would also like to thank the members of UCI NLP, Matt Gard- ner, Sebastian Riedel, and Antoine Bosselut for valuable feedback. This material is based upon work sponsored by the DARPA MCS program un- der Contract No. N660011924033 with the United States Office Of Naval Research.'}, Document {'id': 'docuemnt:2010.15980.pdf-3', 'payload': None, 'vector_score': '0.594513297081', 'document_name': '2010.15980.pdf', 'page_number': '3', 'page_text': 'p(y|xprompt) = 3 p([MASK] = w|xprompt) However, writing prompts is not only time consum- ing, but it is not clear that the same phrasing will be effective for every model, nor is it clear what crite- ria determine whether a particular phrasing the best to elicit the desired information. In light of this, we introduce AUTOPROMPT, a method that constructs customized prompts for a specific task and MLM of interest, to cause the MLMs to produce the desired knowledge.1 An illustration of AUTOPROMPT is provided in Figure 1. The prompt is constructed by taking the original task inputs—a collection of one or more sequences of tokens (e.g., the review in Figure 1)—and mapping them to a sequence of tokens using a template. In the following sections, we describe how AUTOPROMPT uses labeled train- ing data to construct prompts, and how it uses the output of the MLM as a prediction for the task. 2.1 Background and Notation For the purpose of prompt construction, we distin- guish the original task inputs xinp (e.g., the review in Figure 1, “a real joy.”) from the prompt xprompt (e.g., “a real joy. atmosphere alot dialogue Clone totally [MASK].”) that is fed into the MLM. The mapping from xinp to xprompt is performed using a template, λ. This template defines where each input sequence will be placed in the prompt, as well as the placement of any additional tokens. In particular, it must also define the placement of a special [MASK] token for the MLM to fill in (de- noted by [P] in the template to distinguish it from other [MASK] tokens that might appear). Feeding the prompt into the MLM produces a probability distribution p([MASK]|xprompt) describing which tokens most likely fill in the blank. If class labels naturally correspond to tokens in the vocabulary (e.g., entity names in knowledge base completion tasks), this distribution may be readily interpreted as a distribution over class la- bels. However, for tasks such as sentiment analysis, there may be a set of label tokens Vy that corre- spond to a particular label y. For example, in Fig- ure 1, “Cris”, “marvelous”, and “philanthrop” all indicate positive sentiment. In this case, the class probability is obtained by marginalizing over the 1Although we focus only on MLMs in this work, our method is trivially extendable to autoregressive LMs. The only adjustment is that the predict token must occur at the end of the prompt. set of label tokens: w∈Vy (1) 2.2 Gradient-Based Prompt Search So far, we have shown how to reformulate a clas- sification task as a language modeling task using prompts. Here, we propose a method for automatic prompt construction based on Wallace et al. (2019). The idea is to add a number of “trigger” tokens that are shared across all prompts (denoted by [T] in the example template in Figure 1). These tokens are initialized to [MASK] tokens, and then iteratively updated to maximize the label likelihood (Equa- tion (1)) over batches of examples. Formally, at each step, we compute a first-order approximation of the change in the log-likelihood that would be produced by swapping the jth trigger token x (j) trig with another token w ∈ V. Then we identify a candidate set Vcand of the top-k tokens estimated to cause the greatest increase: Vcand = top-k wT in∇ log p(y|xprompt) لا (2) w∈V where win is the input embedding of w, and the gradient is taken with respect to the input embed- ding of x (j) trig. Note that computing this candidate set is roughly as expensive as a single forward pass and backward pass of the model (the dot-products require the same amount of multiplications as com- puting the LM output projection). For each candi- date in this set, we then re-evaluate Equation (1) on the updated prompt, and retain the prompt with the highest probability in the next step—this requires k forward passes of the model. An example prompt produced by this method for the task of sentiment analysis is shown in Figure 1. 2.3 Automating Label Token Selection While in some settings the choice of label tokens is obvious (e.g., when class labels directly correspond to words in the vocabulary), it is less clear what label tokens are appropriate for problems involv- ing more abstract class labels (e.g., NLI). In this section, we develop a general two-step approach to automate the selection of the sets of label tokens Vy. In the first step, we train a logistic classifier to predict the class label using the contextualized embedding of the [MASK] token as input: h = Transformerenc(x˜) (3)'}]}\n",
      "[Document(page_content='A PROMPT ENGINEERING IN THE WILD Large models with natural language interfaces, including models for text generation and image synthesis, have seen an increasing amount of public usage in recent years. As finding the right prompt can be difficult for humans, a number of guides on prompt engineering as well as tools to aid in prompt discovery have been developed. Among others, see, for example: • https://blog.andrewcantino.com/blog/2021/04/21/prompt-engineering-tips-and-tricks/ • https://techcrunch.com/2022/07/29/a-startup-is-charging-1-99-for-strings-of-text-to-feed-to-dall-e-2/ • https://news.ycombinator.com/item?id=32943224 • https://promptomania.com/stable-diffusion-prompt-builder/ • https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion In this paper we apply APE to generate effective instructions for steering LLMs, but the general framework Algorithm 1 could be applied to steer other models with natural language interfaces so long as an appropriate proposal method and scoring function can be designed. 15', lookup_str='', metadata={'source': '2211.01910.pdf:15'}, lookup_index=0), Document(page_content='LARGE LANGUAGE PROMPT ENGINEERS MODELS ARE HUMAN-LEVEL Yongchao Zhou∗,1,2 Andrei Ioan Muresanu∗,2,3 Silviu Pitis1,2 Harris Chan1,2 1University of Toronto 2Vector Institute 3University of Waterloo ABSTRACT Ziwen Han∗,1,2 Jimmy Ba1,2 Keiran Paster1,2 By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the “program,” optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer. 1 1 INTRODUCTION The combination of scale and attention-based architectures has resulted in language models possessing an unprecedented level of generality (Kaplan et al., 2020; Vaswani et al., 2017). These so-called “large language models” (LLMs) have shown remarkable, often superhuman, capabilities across a diverse range of tasks, including both zero-shot and few-shot setups (Brown et al., 2020; Srivastava et al., 2022). With generality, however, there comes a question of control: how can we make LLMs do what we want them to do? To answer this question and steer LLMs toward desired behaviors, recent work has considered fine-tuning (Ouyang et al., 2022; Ziegler et al., 2019), in-context learning (Brown et al., 2020), and several forms of prompt generation (Gao, 2021), including both differentiable tuning of soft prompts (Qin & Eisner, 2021; Lester et al., 2021) and natural language prompt engineering (Reynolds & McDonell, 2021). The latter is of particular interest, as it provides a natural interface for humans to communicate with machines and may be of great relevance not only to LLMs but to other generalist models such as prompted image synthesizers (Rombach et al., 2022; Ramesh et al., 2022), for which public interest in prompt design and generation has also emerged (see Appendix A for examples). Behind this interest is the fact that plain language prompts do not always produce the desired results, even when those results are possible to produce with alternative instructions. Thus, human users must experiment with a wide range of prompts to elicit desired behaviors, as they have little knowledge of how compatible instructions are with a particular model. We can understand this by viewing LLMs as black-box computers that execute programs specified by natural language instructions: while they can execute a broad range of natural language programs, the way these programs are processed may not be intuitive for humans, and the quality of instruction can only be measured when executing these instructions on a downstream task (Sanh et al., 2022; Wei et al., 2021). To reduce the human effort involved in creating and validating effective instructions, we propose a novel algorithm using LLMs to generate and select instructions automatically. We call this problem * Equal contribution. Corresponding email: yczhou@cs.toronto.edu 1 Our code is available at https://github.com/keirp/automatic_prompt_engineer. 1 arXiv:2211.01910v1 [cs.LG] 3 Nov 2022', lookup_str='', metadata={'source': '2211.01910.pdf:1'}, lookup_index=0), Document(page_content='Original Input Cinp a real joy. AUTOPROMPT Iprompt a real joy. atmosphere alot dialogue Clone totally [MASK]. Trigger Tokens Itrig atmosphere, alot, dialogue, Clone... Masked LM p([MASK]|æprompt) Cris marvelous philanthrop Template X(Kinp, \"\\'trig) {sentence}[T][T][T][T][T][P]. p(y x prompt) + positive worse incompetence Worse + negative Figure 1: Illustration of AUTOPROMPT applied to probe a masked language model’s (MLM’s) ability to per- form sentiment analysis. Each input, xinp, is placed into a natural language prompt, xprompt, which contains a single [MASK] token. The prompt is created using a template, λ, which combines the original input with a set of trigger tokens, xtrig. The trigger tokens are shared across all inputs and determined using a gradient-based search (Section 2.2). Probabilities for each class label, y, are then obtained by marginalizing the MLM predictions, p([MASK]|xprompt), over sets of automatically detected label tokens (Section 2.3). vides a lower bound on what the model “knows”, and is therefore a more useful analysis tool. How- ever, prompting unfortunately requires manually crafting the context to feed into the model. Not only is this time consuming and non-intuitive for many tasks (e.g., textual entailment), more impor- tantly, models are highly sensitive to this context: improperly-constructed contexts cause artificially low performance (Jiang et al., 2020). Overcoming the need to manually specify prompts would make prompting a more widely useful analysis tool. In this paper, we introduce AUTOPROMPT—an automated method for generating prompts for any task, illustrated in Figure 1. Given a task, e.g., sen- timent analysis, AUTOPROMPT creates a prompt by combining the original task inputs (e.g. reviews) with a collection of trigger tokens according to a template. The same set of trigger tokens is used for all inputs, and is learned using a variant of the gradient-based search strategy proposed in Wallace et al. (2019). The LM predictions for the prompt are converted to class probabilities by marginal- izing over a set of associated label tokens, which can either be learned or specified ahead of time, enabling the LM to be evaluated the same as one would any other classifier. We validate the effectiveness of AUTOPROMPT in numerous experiments. First, we use AUTO- PROMPT to construct prompts that test pretrained masked language models (MLMs) on sentiment analysis and natural language inference (NLI). Our tests reveal that, without any finetuning, MLMs perform well on both of these tasks—a properly- prompted RoBERTa achieves 91% accuracy on SST-2 (better than a finetuned ELMo model (Pe- ters et al., 2018)), and 69% accuracy on a bal- anced variant of the SICK-E dataset (Marelli et al., 2014). Next, we apply AUTOPROMPT to the fact re- trieval tasks of LAMA (Petroni et al., 2019), where we are able to construct prompts that more effec- tively elicit MLM’s factual knowledge than exist- ing prompts generated using manual and corpus- mining methods. Concretely, we achieve 43.3% precision-at-1, compared to the current best single- prompt result of 34.1% (Jiang et al., 2020). We also introduce a variant of this task, similar to rela- tion extraction (RE), that tests whether MLMs can extract knowledge from a given piece of text. We show that MLMs can actually outperform existing RE models when context sentences with real facts are provided, however, they struggle when context sentences are artificially falsified. Finally, although the goal of AUTOPROMPT is to analyze models, we find that it provides certain practical advantages over finetuning. First, AU- TOPROMPT achieves higher average- and worst- case accuracy than finetuning in low-data regimes. Moreover, unlike finetuning, prompting LMs does not require large amounts of disk space to store model checkpoints; once a prompt is found, it can be used on off-the-shelf pretrained LMs. This is beneficial when serving models for multiple tasks. 2 Overview of AUTOPROMPT A natural way to elicit knowledge from pretrained LMs is to pose tasks as fill-in-the-blank problems.', lookup_str='', metadata={'source': '2010.15980.pdf:2'}, lookup_index=0), Document(page_content='Model Perturbed Original Supervised RE LSTM 57.95 90.73 56.43 BERT (AUTOPROMPT) RoBERTa (AUTOPROMPT) 60.33 28.95 58.81 BERT (LAMA) 69.06 28.02 BERT (LPAQA) 76.55 30.79 Table 5: Relation Extraction: We use prompts to test pretrained MLMs on relation extraction. Compared to a state-of-the-art LSTM model from 2017, MLMs have higher mean precision-at-1 (P@1), especially when us- ing prompts from AUTOPROMPT. We also test models on sentences that have been edited to contain incorrect facts. The accuracy of MLMs drops significantly on these sentences, indicating that their high performance stems from their factual knowledge. 7 Discussion Prompting as an Alternative to Finetuning The goal of prompting a language model is to probe the knowledge that the model acquired from pre- training. Nevertheless, prompting has some prac- tical advantages over finetuning for solving real- world tasks. First, as shown in Section 3, prompts generated using AUTOPROMPT can achieve higher accuracy than finetuning in the low-data regime. Moreover, prompting has advantages over finetun- ing when trying to solve many different tasks (e.g., the many users of the OpenAI GPT-3 API (Brown et al., 2020)). In particular, finetuning requires storing large language model checkpoints for each individual task, and drastically increases system cost and complexity because it requires deploying many different models at the same time. Prompt- ing alleviates both of these issues. Only prompts are stored for each individual task, while the same pretrained model is used across all of the tasks. Limitations of Prompting There are certain phenomena that are difficult to elicit from pre- trained language models via prompts. In our pre- liminary evaluation on datasets such as QQP (Iyer et al., 2017) and RTE (Dagan et al., 2005), prompts generated manually and with AUTOPROMPT did not perform considerably better than chance. How- ever, we cannot conclude that BERT does not know paraphrasing or entailment from these results. In general, different probing methods have different tasks and phenomena they are suitable for: AUTO- PROMPT makes prompt-based probes more gener- ally applicable, but, it still remains just one tool in the toolbox of the interpretability researcher. Limitations of AUTOPROMPT One downside of AUTOPROMPT is that it requires labeled train- ing data. Although this is also required for other probing techniques (e.g., linear probing classi- fiers), manual prompts rely on domain/language insights instead of labeled data. Compared to human-designed prompts, AUTOPROMPT gener- ated prompts lack interpretability, which is similar to other probing techniques, such as linear probing classifiers. Another limitation of AUTOPROMPT is that it can sometimes struggle when the training data is highly imbalanced. For example, in Sec- tions 4 and 5 we show that the prompts often just increase the likelihood of the majority label. Re- balancing the training data can help to mitigate this problem. Finally, due to the greedy search over the large discrete space of phrases, AUTOPROMPT is sometimes brittle; we leave more effective crafting techniques for future directions. 8 Conclusion In this paper, we introduce AUTOPROMPT, an approach to develop automatically-constructed prompts that elicit knowledge from pretrained MLMs for a variety of tasks. We show that these prompts outperform manual prompts while requir- ing less human effort. Furthermore, the results for sentiment analysis and textual entailment sug- gest that, in some data-scarce settings, it may be more effective to prompt language models than to finetune them for the task. Although we fo- cus only on masked language models in this paper, our method can be trivially extended to standard language models, and thus maybe useful for con- structing inputs for models like GPT-3 (Brown et al., 2020). Source code and datasets to re- produce the results in this paper is available at http://ucinlp.github.io/autoprompt. Acknowledgments We would like to thank the LAMA and LPAQA teams for answering our questions. We would also like to thank the members of UCI NLP, Matt Gard- ner, Sebastian Riedel, and Antoine Bosselut for valuable feedback. This material is based upon work sponsored by the DARPA MCS program un- der Contract No. N660011924033 with the United States Office Of Naval Research.', lookup_str='', metadata={'source': '2010.15980.pdf:9'}, lookup_index=0), Document(page_content='p(y|xprompt) = 3 p([MASK] = w|xprompt) However, writing prompts is not only time consum- ing, but it is not clear that the same phrasing will be effective for every model, nor is it clear what crite- ria determine whether a particular phrasing the best to elicit the desired information. In light of this, we introduce AUTOPROMPT, a method that constructs customized prompts for a specific task and MLM of interest, to cause the MLMs to produce the desired knowledge.1 An illustration of AUTOPROMPT is provided in Figure 1. The prompt is constructed by taking the original task inputs—a collection of one or more sequences of tokens (e.g., the review in Figure 1)—and mapping them to a sequence of tokens using a template. In the following sections, we describe how AUTOPROMPT uses labeled train- ing data to construct prompts, and how it uses the output of the MLM as a prediction for the task. 2.1 Background and Notation For the purpose of prompt construction, we distin- guish the original task inputs xinp (e.g., the review in Figure 1, “a real joy.”) from the prompt xprompt (e.g., “a real joy. atmosphere alot dialogue Clone totally [MASK].”) that is fed into the MLM. The mapping from xinp to xprompt is performed using a template, λ. This template defines where each input sequence will be placed in the prompt, as well as the placement of any additional tokens. In particular, it must also define the placement of a special [MASK] token for the MLM to fill in (de- noted by [P] in the template to distinguish it from other [MASK] tokens that might appear). Feeding the prompt into the MLM produces a probability distribution p([MASK]|xprompt) describing which tokens most likely fill in the blank. If class labels naturally correspond to tokens in the vocabulary (e.g., entity names in knowledge base completion tasks), this distribution may be readily interpreted as a distribution over class la- bels. However, for tasks such as sentiment analysis, there may be a set of label tokens Vy that corre- spond to a particular label y. For example, in Fig- ure 1, “Cris”, “marvelous”, and “philanthrop” all indicate positive sentiment. In this case, the class probability is obtained by marginalizing over the 1Although we focus only on MLMs in this work, our method is trivially extendable to autoregressive LMs. The only adjustment is that the predict token must occur at the end of the prompt. set of label tokens: w∈Vy (1) 2.2 Gradient-Based Prompt Search So far, we have shown how to reformulate a clas- sification task as a language modeling task using prompts. Here, we propose a method for automatic prompt construction based on Wallace et al. (2019). The idea is to add a number of “trigger” tokens that are shared across all prompts (denoted by [T] in the example template in Figure 1). These tokens are initialized to [MASK] tokens, and then iteratively updated to maximize the label likelihood (Equa- tion (1)) over batches of examples. Formally, at each step, we compute a first-order approximation of the change in the log-likelihood that would be produced by swapping the jth trigger token x (j) trig with another token w ∈ V. Then we identify a candidate set Vcand of the top-k tokens estimated to cause the greatest increase: Vcand = top-k wT in∇ log p(y|xprompt) لا (2) w∈V where win is the input embedding of w, and the gradient is taken with respect to the input embed- ding of x (j) trig. Note that computing this candidate set is roughly as expensive as a single forward pass and backward pass of the model (the dot-products require the same amount of multiplications as com- puting the LM output projection). For each candi- date in this set, we then re-evaluate Equation (1) on the updated prompt, and retain the prompt with the highest probability in the next step—this requires k forward passes of the model. An example prompt produced by this method for the task of sentiment analysis is shown in Figure 1. 2.3 Automating Label Token Selection While in some settings the choice of label tokens is obvious (e.g., when class labels directly correspond to words in the vocabulary), it is less clear what label tokens are appropriate for problems involv- ing more abstract class labels (e.g., NLI). In this section, we develop a general two-step approach to automate the selection of the sets of label tokens Vy. In the first step, we train a logistic classifier to predict the class label using the contextualized embedding of the [MASK] token as input: h = Transformerenc(x˜) (3)', lookup_str='', metadata={'source': '2010.15980.pdf:3'}, lookup_index=0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='A PROMPT ENGINEERING IN THE WILD Large models with natural language interfaces, including models for text generation and image synthesis, have seen an increasing amount of public usage in recent years. As finding the right prompt can be difficult for humans, a number of guides on prompt engineering as well as tools to aid in prompt discovery have been developed. Among others, see, for example: • https://blog.andrewcantino.com/blog/2021/04/21/prompt-engineering-tips-and-tricks/ • https://techcrunch.com/2022/07/29/a-startup-is-charging-1-99-for-strings-of-text-to-feed-to-dall-e-2/ • https://news.ycombinator.com/item?id=32943224 • https://promptomania.com/stable-diffusion-prompt-builder/ • https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion In this paper we apply APE to generate effective instructions for steering LLMs, but the general framework Algorithm 1 could be applied to steer other models with natural language interfaces so long as an appropriate proposal method and scoring function can be designed. 15', lookup_str='', metadata={'source': '2211.01910.pdf:15'}, lookup_index=0),\n",
       "  Document(page_content='LARGE LANGUAGE PROMPT ENGINEERS MODELS ARE HUMAN-LEVEL Yongchao Zhou∗,1,2 Andrei Ioan Muresanu∗,2,3 Silviu Pitis1,2 Harris Chan1,2 1University of Toronto 2Vector Institute 3University of Waterloo ABSTRACT Ziwen Han∗,1,2 Jimmy Ba1,2 Keiran Paster1,2 By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the “program,” optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer. 1 1 INTRODUCTION The combination of scale and attention-based architectures has resulted in language models possessing an unprecedented level of generality (Kaplan et al., 2020; Vaswani et al., 2017). These so-called “large language models” (LLMs) have shown remarkable, often superhuman, capabilities across a diverse range of tasks, including both zero-shot and few-shot setups (Brown et al., 2020; Srivastava et al., 2022). With generality, however, there comes a question of control: how can we make LLMs do what we want them to do? To answer this question and steer LLMs toward desired behaviors, recent work has considered fine-tuning (Ouyang et al., 2022; Ziegler et al., 2019), in-context learning (Brown et al., 2020), and several forms of prompt generation (Gao, 2021), including both differentiable tuning of soft prompts (Qin & Eisner, 2021; Lester et al., 2021) and natural language prompt engineering (Reynolds & McDonell, 2021). The latter is of particular interest, as it provides a natural interface for humans to communicate with machines and may be of great relevance not only to LLMs but to other generalist models such as prompted image synthesizers (Rombach et al., 2022; Ramesh et al., 2022), for which public interest in prompt design and generation has also emerged (see Appendix A for examples). Behind this interest is the fact that plain language prompts do not always produce the desired results, even when those results are possible to produce with alternative instructions. Thus, human users must experiment with a wide range of prompts to elicit desired behaviors, as they have little knowledge of how compatible instructions are with a particular model. We can understand this by viewing LLMs as black-box computers that execute programs specified by natural language instructions: while they can execute a broad range of natural language programs, the way these programs are processed may not be intuitive for humans, and the quality of instruction can only be measured when executing these instructions on a downstream task (Sanh et al., 2022; Wei et al., 2021). To reduce the human effort involved in creating and validating effective instructions, we propose a novel algorithm using LLMs to generate and select instructions automatically. We call this problem * Equal contribution. Corresponding email: yczhou@cs.toronto.edu 1 Our code is available at https://github.com/keirp/automatic_prompt_engineer. 1 arXiv:2211.01910v1 [cs.LG] 3 Nov 2022', lookup_str='', metadata={'source': '2211.01910.pdf:1'}, lookup_index=0),\n",
       "  Document(page_content='Original Input Cinp a real joy. AUTOPROMPT Iprompt a real joy. atmosphere alot dialogue Clone totally [MASK]. Trigger Tokens Itrig atmosphere, alot, dialogue, Clone... Masked LM p([MASK]|æprompt) Cris marvelous philanthrop Template X(Kinp, \"\\'trig) {sentence}[T][T][T][T][T][P]. p(y x prompt) + positive worse incompetence Worse + negative Figure 1: Illustration of AUTOPROMPT applied to probe a masked language model’s (MLM’s) ability to per- form sentiment analysis. Each input, xinp, is placed into a natural language prompt, xprompt, which contains a single [MASK] token. The prompt is created using a template, λ, which combines the original input with a set of trigger tokens, xtrig. The trigger tokens are shared across all inputs and determined using a gradient-based search (Section 2.2). Probabilities for each class label, y, are then obtained by marginalizing the MLM predictions, p([MASK]|xprompt), over sets of automatically detected label tokens (Section 2.3). vides a lower bound on what the model “knows”, and is therefore a more useful analysis tool. How- ever, prompting unfortunately requires manually crafting the context to feed into the model. Not only is this time consuming and non-intuitive for many tasks (e.g., textual entailment), more impor- tantly, models are highly sensitive to this context: improperly-constructed contexts cause artificially low performance (Jiang et al., 2020). Overcoming the need to manually specify prompts would make prompting a more widely useful analysis tool. In this paper, we introduce AUTOPROMPT—an automated method for generating prompts for any task, illustrated in Figure 1. Given a task, e.g., sen- timent analysis, AUTOPROMPT creates a prompt by combining the original task inputs (e.g. reviews) with a collection of trigger tokens according to a template. The same set of trigger tokens is used for all inputs, and is learned using a variant of the gradient-based search strategy proposed in Wallace et al. (2019). The LM predictions for the prompt are converted to class probabilities by marginal- izing over a set of associated label tokens, which can either be learned or specified ahead of time, enabling the LM to be evaluated the same as one would any other classifier. We validate the effectiveness of AUTOPROMPT in numerous experiments. First, we use AUTO- PROMPT to construct prompts that test pretrained masked language models (MLMs) on sentiment analysis and natural language inference (NLI). Our tests reveal that, without any finetuning, MLMs perform well on both of these tasks—a properly- prompted RoBERTa achieves 91% accuracy on SST-2 (better than a finetuned ELMo model (Pe- ters et al., 2018)), and 69% accuracy on a bal- anced variant of the SICK-E dataset (Marelli et al., 2014). Next, we apply AUTOPROMPT to the fact re- trieval tasks of LAMA (Petroni et al., 2019), where we are able to construct prompts that more effec- tively elicit MLM’s factual knowledge than exist- ing prompts generated using manual and corpus- mining methods. Concretely, we achieve 43.3% precision-at-1, compared to the current best single- prompt result of 34.1% (Jiang et al., 2020). We also introduce a variant of this task, similar to rela- tion extraction (RE), that tests whether MLMs can extract knowledge from a given piece of text. We show that MLMs can actually outperform existing RE models when context sentences with real facts are provided, however, they struggle when context sentences are artificially falsified. Finally, although the goal of AUTOPROMPT is to analyze models, we find that it provides certain practical advantages over finetuning. First, AU- TOPROMPT achieves higher average- and worst- case accuracy than finetuning in low-data regimes. Moreover, unlike finetuning, prompting LMs does not require large amounts of disk space to store model checkpoints; once a prompt is found, it can be used on off-the-shelf pretrained LMs. This is beneficial when serving models for multiple tasks. 2 Overview of AUTOPROMPT A natural way to elicit knowledge from pretrained LMs is to pose tasks as fill-in-the-blank problems.', lookup_str='', metadata={'source': '2010.15980.pdf:2'}, lookup_index=0),\n",
       "  Document(page_content='Model Perturbed Original Supervised RE LSTM 57.95 90.73 56.43 BERT (AUTOPROMPT) RoBERTa (AUTOPROMPT) 60.33 28.95 58.81 BERT (LAMA) 69.06 28.02 BERT (LPAQA) 76.55 30.79 Table 5: Relation Extraction: We use prompts to test pretrained MLMs on relation extraction. Compared to a state-of-the-art LSTM model from 2017, MLMs have higher mean precision-at-1 (P@1), especially when us- ing prompts from AUTOPROMPT. We also test models on sentences that have been edited to contain incorrect facts. The accuracy of MLMs drops significantly on these sentences, indicating that their high performance stems from their factual knowledge. 7 Discussion Prompting as an Alternative to Finetuning The goal of prompting a language model is to probe the knowledge that the model acquired from pre- training. Nevertheless, prompting has some prac- tical advantages over finetuning for solving real- world tasks. First, as shown in Section 3, prompts generated using AUTOPROMPT can achieve higher accuracy than finetuning in the low-data regime. Moreover, prompting has advantages over finetun- ing when trying to solve many different tasks (e.g., the many users of the OpenAI GPT-3 API (Brown et al., 2020)). In particular, finetuning requires storing large language model checkpoints for each individual task, and drastically increases system cost and complexity because it requires deploying many different models at the same time. Prompt- ing alleviates both of these issues. Only prompts are stored for each individual task, while the same pretrained model is used across all of the tasks. Limitations of Prompting There are certain phenomena that are difficult to elicit from pre- trained language models via prompts. In our pre- liminary evaluation on datasets such as QQP (Iyer et al., 2017) and RTE (Dagan et al., 2005), prompts generated manually and with AUTOPROMPT did not perform considerably better than chance. How- ever, we cannot conclude that BERT does not know paraphrasing or entailment from these results. In general, different probing methods have different tasks and phenomena they are suitable for: AUTO- PROMPT makes prompt-based probes more gener- ally applicable, but, it still remains just one tool in the toolbox of the interpretability researcher. Limitations of AUTOPROMPT One downside of AUTOPROMPT is that it requires labeled train- ing data. Although this is also required for other probing techniques (e.g., linear probing classi- fiers), manual prompts rely on domain/language insights instead of labeled data. Compared to human-designed prompts, AUTOPROMPT gener- ated prompts lack interpretability, which is similar to other probing techniques, such as linear probing classifiers. Another limitation of AUTOPROMPT is that it can sometimes struggle when the training data is highly imbalanced. For example, in Sec- tions 4 and 5 we show that the prompts often just increase the likelihood of the majority label. Re- balancing the training data can help to mitigate this problem. Finally, due to the greedy search over the large discrete space of phrases, AUTOPROMPT is sometimes brittle; we leave more effective crafting techniques for future directions. 8 Conclusion In this paper, we introduce AUTOPROMPT, an approach to develop automatically-constructed prompts that elicit knowledge from pretrained MLMs for a variety of tasks. We show that these prompts outperform manual prompts while requir- ing less human effort. Furthermore, the results for sentiment analysis and textual entailment sug- gest that, in some data-scarce settings, it may be more effective to prompt language models than to finetune them for the task. Although we fo- cus only on masked language models in this paper, our method can be trivially extended to standard language models, and thus maybe useful for con- structing inputs for models like GPT-3 (Brown et al., 2020). Source code and datasets to re- produce the results in this paper is available at http://ucinlp.github.io/autoprompt. Acknowledgments We would like to thank the LAMA and LPAQA teams for answering our questions. We would also like to thank the members of UCI NLP, Matt Gard- ner, Sebastian Riedel, and Antoine Bosselut for valuable feedback. This material is based upon work sponsored by the DARPA MCS program un- der Contract No. N660011924033 with the United States Office Of Naval Research.', lookup_str='', metadata={'source': '2010.15980.pdf:9'}, lookup_index=0),\n",
       "  Document(page_content='p(y|xprompt) = 3 p([MASK] = w|xprompt) However, writing prompts is not only time consum- ing, but it is not clear that the same phrasing will be effective for every model, nor is it clear what crite- ria determine whether a particular phrasing the best to elicit the desired information. In light of this, we introduce AUTOPROMPT, a method that constructs customized prompts for a specific task and MLM of interest, to cause the MLMs to produce the desired knowledge.1 An illustration of AUTOPROMPT is provided in Figure 1. The prompt is constructed by taking the original task inputs—a collection of one or more sequences of tokens (e.g., the review in Figure 1)—and mapping them to a sequence of tokens using a template. In the following sections, we describe how AUTOPROMPT uses labeled train- ing data to construct prompts, and how it uses the output of the MLM as a prediction for the task. 2.1 Background and Notation For the purpose of prompt construction, we distin- guish the original task inputs xinp (e.g., the review in Figure 1, “a real joy.”) from the prompt xprompt (e.g., “a real joy. atmosphere alot dialogue Clone totally [MASK].”) that is fed into the MLM. The mapping from xinp to xprompt is performed using a template, λ. This template defines where each input sequence will be placed in the prompt, as well as the placement of any additional tokens. In particular, it must also define the placement of a special [MASK] token for the MLM to fill in (de- noted by [P] in the template to distinguish it from other [MASK] tokens that might appear). Feeding the prompt into the MLM produces a probability distribution p([MASK]|xprompt) describing which tokens most likely fill in the blank. If class labels naturally correspond to tokens in the vocabulary (e.g., entity names in knowledge base completion tasks), this distribution may be readily interpreted as a distribution over class la- bels. However, for tasks such as sentiment analysis, there may be a set of label tokens Vy that corre- spond to a particular label y. For example, in Fig- ure 1, “Cris”, “marvelous”, and “philanthrop” all indicate positive sentiment. In this case, the class probability is obtained by marginalizing over the 1Although we focus only on MLMs in this work, our method is trivially extendable to autoregressive LMs. The only adjustment is that the predict token must occur at the end of the prompt. set of label tokens: w∈Vy (1) 2.2 Gradient-Based Prompt Search So far, we have shown how to reformulate a clas- sification task as a language modeling task using prompts. Here, we propose a method for automatic prompt construction based on Wallace et al. (2019). The idea is to add a number of “trigger” tokens that are shared across all prompts (denoted by [T] in the example template in Figure 1). These tokens are initialized to [MASK] tokens, and then iteratively updated to maximize the label likelihood (Equa- tion (1)) over batches of examples. Formally, at each step, we compute a first-order approximation of the change in the log-likelihood that would be produced by swapping the jth trigger token x (j) trig with another token w ∈ V. Then we identify a candidate set Vcand of the top-k tokens estimated to cause the greatest increase: Vcand = top-k wT in∇ log p(y|xprompt) لا (2) w∈V where win is the input embedding of w, and the gradient is taken with respect to the input embed- ding of x (j) trig. Note that computing this candidate set is roughly as expensive as a single forward pass and backward pass of the model (the dot-products require the same amount of multiplications as com- puting the LM output projection). For each candi- date in this set, we then re-evaluate Equation (1) on the updated prompt, and retain the prompt with the highest probability in the next step—this requires k forward passes of the model. An example prompt produced by this method for the task of sentiment analysis is shown in Figure 1. 2.3 Automating Label Token Selection While in some settings the choice of label tokens is obvious (e.g., when class labels directly correspond to words in the vocabulary), it is less clear what label tokens are appropriate for problems involv- ing more abstract class labels (e.g., NLI). In this section, we develop a general two-step approach to automate the selection of the sets of label tokens Vy. In the first step, we train a logistic classifier to predict the class label using the contextualized embedding of the [MASK] token as input: h = Transformerenc(x˜) (3)', lookup_str='', metadata={'source': '2010.15980.pdf:3'}, lookup_index=0)],\n",
       " 'question': 'How does automated prompt engineering work?',\n",
       " 'source': '2010.15980.pdf:2',\n",
       " 'intermediate_steps': [{'answer': ' Automated prompt engineering (APE) is a general framework for generating effective instructions for steering large language models with natural language interfaces. It involves designing an appropriate proposal method and scoring function, and then applying the framework (Algorithm 1) to steer the model.',\n",
       "   'score': '80'},\n",
       "  {'answer': ' Automated prompt engineering is a novel algorithm that uses large language models (LLMs) to generate and select instructions automatically. The instructions are treated as \"programs\" and optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. The quality of the instruction is evaluated by evaluating the zero-shot performance of another LLM following the selected instruction.',\n",
       "   'score': '80'},\n",
       "  {'answer': ' Automated prompt engineering works by combining the original task inputs with a collection of trigger tokens according to a template. The same set of trigger tokens is used for all inputs, and is learned using a variant of the gradient-based search strategy proposed in Wallace et al. (2019). The LM predictions for the prompt are converted to class probabilities by marginalizing over a set of associated label tokens, which can either be learned or specified ahead of time, enabling the LM to be evaluated the same as one would any other classifier.',\n",
       "   'score': '90'},\n",
       "  {'answer': ' Automated prompt engineering is a method to develop automatically-constructed prompts that elicit knowledge from pretrained MLMs for a variety of tasks. It works by using a greedy search over a large discrete space of phrases to generate prompts that can be used to probe the knowledge that the model acquired from pre-training.',\n",
       "   'score': '80'},\n",
       "  {'answer': ' Automated prompt engineering works by taking the original task inputs (e.g., a collection of one or more sequences of tokens) and mapping them to a sequence of tokens using a template. The template defines where each input sequence will be placed in the prompt, as well as the placement of any additional tokens. The prompt is then fed into the MLM to produce a probability distribution p([MASK]|xprompt) describing which tokens most likely fill in the blank. A logistic classifier is then trained to predict the class label using the contextualized embedding of the [MASK] token as input.',\n",
       "   'score': '90'}],\n",
       " 'output_text': ' Automated prompt engineering works by combining the original task inputs with a collection of trigger tokens according to a template. The same set of trigger tokens is used for all inputs, and is learned using a variant of the gradient-based search strategy proposed in Wallace et al. (2019). The LM predictions for the prompt are converted to class probabilities by marginalizing over a set of associated label tokens, which can either be learned or specified ahead of time, enabling the LM to be evaluated the same as one would any other classifier.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_emb.query('How does automated prompt engineering work?', topK=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
