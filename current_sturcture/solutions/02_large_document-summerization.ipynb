{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Summerization\n",
    "\n",
    "Process:\n",
    "\n",
    "1. Generate summary for first page\n",
    "2. Refine summary with second page\n",
    "3. Repeate #2 for all pages\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Enviroments for following\n",
    "\n",
    "- Azure Form Recognizer Endpoint & Key\n",
    "- Azure OpenAI endpoint & key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AzureOpenAIUtil.AzureFormRecognizer import AzureFormRecognizerRead\n",
    "from AzureOpenAIUtil.Summerization import summarization\n",
    "\n",
    "# File Directories \n",
    "# -- raw data\n",
    "RAW_DATA_FOLDER= 'data/raw'\n",
    "# -- extracted json file \n",
    "EXTRACTED_DATA_FOLDER = 'data/extracted'\n",
    "# -- summary files\n",
    "SUMMARY_FOLDER = 'data/summary'\n",
    "# Azure Form Recognizer Read\n",
    "azure_fr = AzureFormRecognizerRead()\n",
    "# Azure OpenAI util for summerization\n",
    "summary_eng = summarization(deployment_name=os.getenv('DEPLOYMENT_NAME'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run From Recognizer to OCR documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 2010.15980.pdf    write output to data/extracted/2010.15980.json\n",
      "Processing file: 2101.00190.pdf    write output to data/extracted/2101.00190.json\n",
      "Processing file: 2104.08691.pdf    write output to data/extracted/2104.08691.json\n",
      "Processing file: 2110.08387.pdf    write output to data/extracted/2110.08387.json\n",
      "Processing file: 2201.11903.pdf    write output to data/extracted/2201.11903.json\n",
      "Processing file: 2203.11171.pdf    write output to data/extracted/2203.11171.json\n",
      "Processing file: 2211.01910.pdf    write output to data/extracted/2211.01910.json\n",
      "Processing file: 2212.10496.pdf    write output to data/extracted/2212.10496.json\n"
     ]
    }
   ],
   "source": [
    "azure_fr.extract_files(RAW_DATA_FOLDER, EXTRACTED_DATA_FOLDER)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Summaries for extracted documents\n",
    "\n",
    "In this demo, we show how to generate one short summary for each document.\n",
    "\n",
    "*ignore the warning of input being truncated may show in the output - it's caused by GPT2 tokenizer for sequence longer than 1024*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generate summary for file: 2010.15980.json\n",
      "{'file': '2010.15980.json', 'summerization_type': 'map_reduce', 'summary': ' This paper introduces AUTOPROMPT, an automated method for generating prompts for tasks such as sentiment analysis, natural language inference, fact retrieval, and relation extraction. Experiments show that AUTOPROMPT outperforms linear probes and GLUE leaderboard scores on the Stanford Sentiment Treebank (SST-2) task, and that it can be a viable alternative to finetuning in the low-data regime. Examples of prompts generated using AUTOPROMPT are provided, and the results of the tests are also presented.'}\n",
      "\n",
      "\n",
      "Generate summary for file: 2101.00190.json\n",
      "{'file': '2101.00190.json', 'summerization_type': 'map_reduce', 'summary': ' This paper proposes a lightweight alternative to fine-tuning for natural language generation tasks called prefix-tuning. This approach keeps the language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Results show that prefix-tuning outperforms fine-tuning in low-data settings and is more efficient, preserving the pretrained LM as much as possible. Additionally, it has the potential to scale to even larger models with a similar architecture.'}\n",
      "\n",
      "\n",
      "Generate summary for file: 2104.08691.json\n",
      "{'file': '2104.08691.json', 'summerization_type': 'map_reduce', 'summary': ' This paper explores the use of prompt tuning, a method of adapting pre-trained language models to downstream tasks, to improve language understanding and natural language processing. It compares the performance of prompt tuning to other methods such as model tuning and prompt design, and finds that it is more parameter efficient and robust to domain shifts. It also discusses the hyperparameter search space and provides the mean and standard deviation of the runtime until convergence for various prompt lengths and model sizes.'}\n",
      "\n",
      "\n",
      "Generate summary for file: 2110.08387.json\n",
      "{'file': '2110.08387.json', 'summerization_type': 'map_reduce', 'summary': ' This paper proposes a method for generating knowledge statements from language models to improve the performance of zero-shot and finetuned models on numerical, general, and scientific commonsense benchmarks. Experiments show that the generated knowledge statements can improve the accuracy of the model, and that the quality, quantity, and integration strategy of the knowledge are important factors in the performance. The paper also provides guidelines for human evaluation of knowledge statements and for determining whether a statement is helpful, harmful, or neutral.'}\n",
      "\n",
      "\n",
      "Generate summary for file: 2201.11903.json\n",
      "{'file': '2201.11903.json', 'summerization_type': 'map_reduce', 'summary': ' Geva et al. (2021) explore the use of chain-of-thought prompting to improve the performance of large language models on multi-step reasoning tasks. Results show that chain-of-thought prompting can outperform the prior state of the art on StrategyQA and an unaided sports enthusiast on Sports Understanding. The paper also discusses ethical considerations, experiments conducted on various tasks, version control, reproducibility, computational resources, and examples of correct and incorrect chains of thought produced by LaMDA 137B and PaLM 540B.'}\n",
      "\n",
      "\n",
      "Generate summary for file: 2203.11171.json\n",
      "{'file': '2203.11171.json', 'summerization_type': 'map_reduce', 'summary': '\\n\\nThis paper proposes a self-consistency approach to improve chain of thought reasoning in language models. The approach samples a diverse set of reasoning paths and marginalizes out the sampled reasoning paths to aggregate final answers. Results show that self-consistency significantly improves the accuracy of arithmetic, commonsense, and symbolic reasoning tasks over chain-of-thought prompting, and achieves new state-of-the-art results on almost all tasks. It is also useful for collecting rationales and providing uncertainty estimates.'}\n",
      "\n",
      "\n",
      "Generate summary for file: 2211.01910.json\n",
      "{'file': '2211.01910.json', 'summerization_type': 'map_reduce', 'summary': ' This paper proposes a black-box optimization problem using large language models to generate and search for heuristically viable candidate solutions for natural language program synthesis. Experiments on 24 NLP tasks show that the automatically generated instructions outperform the prior LLM baseline and achieve better or comparable performance to the instructions generated by human annotators. The paper also evaluates the transfer ability of the APE instruction to a different model and finds that the APE instruction is able to transfer with high accuracy.'}\n",
      "\n",
      "\n",
      "Generate summary for file: 2212.10496.json\n",
      "{'file': '2212.10496.json', 'summerization_type': 'map_reduce', 'summary': ' This paper presents the HyDE model, a hybrid deep retrieval model that combines a generative language model and a contrastive encoder to improve retrieval performance. Experiments show that HyDE can outperform non-Contriever models and fine-tuned mContrieverFT models on multilingual retrieval tasks. Additionally, the paper discusses the use of finetuned language models as zero-shot learners, Approximate Nearest Neighbor Negative Contrastive Learning for dense text retrieval, and Combating Distribution Shifts in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning. It also introduces Mr. TyDi, a multi-lingual benchmark for dense retrieval.'}\n"
     ]
    }
   ],
   "source": [
    "doc_sum = summary_eng.generate_summary_files(source_folder=EXTRACTED_DATA_FOLDER, destination_folder=SUMMARY_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
